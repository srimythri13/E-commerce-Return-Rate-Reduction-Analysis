{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# E-commerce Return Risk \u2013 Google Colab Notebook\n", "\n", "This notebook lets you:\n", "1) Upload `orders.csv` and `returns.csv`\n", "2) Clean & merge data, create `Returned` flag\n", "3) Run exploratory analysis (return rates by product & seller)\n", "4) Train a Logistic Regression model\n", "5) Export **high_risk_products.csv** and **item_return_risk_scores.csv** for Power BI\n", "\n", "**Tip:** Run each cell in order (Shift + Enter)."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# STEP 1: Imports\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.compose import ColumnTransformer\n", "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n", "from sklearn.pipeline import Pipeline\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import roc_auc_score, average_precision_score, classification_report\n", "import joblib\n", "from datetime import datetime\n", "print('Libraries imported.')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# STEP 2: Upload CSV files (orders.csv and returns.csv)\n", "from google.colab import files\n", "print('Please upload orders.csv and returns.csv')\n", "uploaded = files.upload()  # Choose both files\n", "\n", "assert 'orders.csv' in uploaded, 'orders.csv was not uploaded.'\n", "assert 'returns.csv' in uploaded, 'returns.csv was not uploaded.'\n", "\n", "orders = pd.read_csv('orders.csv')\n", "returns = pd.read_csv('returns.csv')\n", "print('Files loaded:', orders.shape, returns.shape)\n", "orders.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# STEP 3: Basic validation\n", "print('Orders columns:', list(orders.columns))\n", "print('Returns columns:', list(returns.columns))\n", "\n", "# Expected minimal columns\n", "expected_orders = {'Order Id','Order Date','Delivery Date','Buyer Id','Buyer Name','Product Id','Product Name','Seller Id','Seller Name'}\n", "expected_returns = {'Order Id','Return Date','Product Id','Product Name','Seller Id','Seller Name'}\n", "\n", "missing_o = expected_orders - set(orders.columns)\n", "missing_r = expected_returns - set(returns.columns)\n", "if missing_o:\n", "    print('WARNING: orders.csv missing columns:', missing_o)\n", "if missing_r:\n", "    print('WARNING: returns.csv missing columns:', missing_r)\n", "\n", "orders.isna().sum().sort_values(ascending=False).head(10)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# STEP 4: Clean & merge \u2013 create Returned flag at (Order Id, Product Id) level\n", "orders['Order Date'] = pd.to_datetime(orders['Order Date'], errors='coerce')\n", "orders['Delivery Date'] = pd.to_datetime(orders['Delivery Date'], errors='coerce')\n", "if 'Return Date' in returns.columns:\n", "    returns['Return Date'] = pd.to_datetime(returns['Return Date'], errors='coerce')\n", "\n", "# Deduplicate returns at line-item level, keep latest return per (Order Id, Product Id)\n", "returns_sorted = returns.sort_values(['Order Id','Product Id','Return Date']).drop_duplicates(['Order Id','Product Id'], keep='last')\n", "\n", "# Merge on Order Id + Product Id\n", "merged = orders.merge(\n", "    returns_sorted[['Order Id','Product Id','Return Date']],\n", "    on=['Order Id','Product Id'], how='left', validate='m:1'\n", ")\n", "merged['Returned'] = merged['Return Date'].notna().astype(int)\n", "\n", "# Feature: delivery days\n", "merged['Delivery_Days'] = (merged['Delivery Date'] - merged['Order Date']).dt.days\n", "\n", "print('Rows:', len(merged), '| Return rate:', round(merged['Returned'].mean(), 4))\n", "merged.head()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# STEP 5: Exploratory analysis\n", "overall_rr = merged['Returned'].mean()\n", "print('Overall Return Rate:', round(overall_rr, 4))\n", "\n", "# Top 10 products by return rate (min volume filter)\n", "prod_stats = merged.groupby(['Product Id','Product Name']).agg(\n", "    items=('Returned','size'), rr=('Returned','mean')\n", ").reset_index()\n", "prod_stats = prod_stats[prod_stats['items'] >= 5]  # filter small volume\n", "top_products = prod_stats.sort_values('rr', ascending=False).head(10)\n", "print(top_products)\n", "\n", "# Plot top sellers by return rate (min 10 items)\n", "seller_stats = merged.groupby(['Seller Id','Seller Name']).agg(\n", "    items=('Returned','size'), rr=('Returned','mean')\n", ").reset_index()\n", "seller_stats = seller_stats[seller_stats['items'] >= 10].sort_values('rr', ascending=False).head(10)\n", "ax = seller_stats.plot(kind='bar', x='Seller Name', y='rr', title='Top Sellers by Return Rate (min 10 items)')\n", "plt.xticks(rotation=45, ha='right')\n", "plt.tight_layout()\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# STEP 6: Model \u2013 Logistic Regression (One-Hot for product & seller, plus delivery days)\n", "from sklearn.utils.class_weight import compute_class_weight\n", "\n", "features = ['Product Name','Seller Name','Delivery_Days']\n", "X = merged[features].copy()\n", "y = merged['Returned'].astype(int)\n", "\n", "cat_cols = ['Product Name','Seller Name']\n", "num_cols = ['Delivery_Days']\n", "\n", "preprocess = ColumnTransformer([\n", "    ('num', StandardScaler(), num_cols),\n", "    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)\n", "])\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n", "pipe = Pipeline([\n", "    ('prep', preprocess),\n", "    ('clf', LogisticRegression(max_iter=300, class_weight='balanced'))\n", "])\n", "pipe.fit(X_train, y_train)\n", "\n", "proba_test = pipe.predict_proba(X_test)[:,1]\n", "roc = roc_auc_score(y_test, proba_test)\n", "ap = average_precision_score(y_test, proba_test)\n", "print({'ROC_AUC': round(roc,4), 'PR_AUC': round(ap,4)})\n", "print(classification_report(y_test, (proba_test>=0.5).astype(int)))\n", "\n", "# Save model (optional)\n", "joblib.dump(pipe, 'return_logreg.pkl')\n", "print('Model saved: return_logreg.pkl')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# STEP 7: Score all items & export CSVs\n", "scores = pipe.predict_proba(merged[features])[:,1]\n", "merged['risk_score'] = scores\n", "merged['risk_bucket'] = pd.qcut(merged['risk_score'], q=[0, 0.8, 0.95, 1.0], labels=['Low','Medium','High'])\n", "\n", "# Export item-level scores\n", "item_path = 'item_return_risk_scores.csv'\n", "merged.to_csv(item_path, index=False)\n", "print('Saved', item_path)\n", "\n", "# Product-level high risk summary\n", "prod = merged.groupby(['Product Id','Product Name']).agg(\n", "    avg_risk=('risk_score','mean'),\n", "    items=('risk_score','size'),\n", "    high_share=('risk_bucket', lambda s: (s=='High').mean())\n", ").reset_index().sort_values(['avg_risk','items'], ascending=False)\n", "prod['is_high_risk'] = ((prod['avg_risk']>=0.6) | (prod['high_share']>=0.3)).astype(int)\n", "prod_path = 'high_risk_products.csv'\n", "prod.to_csv(prod_path, index=False)\n", "print('Saved', prod_path)\n", "\n", "# Download files\n", "from google.colab import files\n", "files.download(item_path)\n", "files.download(prod_path)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Next Steps (Optional)\n", "- Add more features if available (price, discount, channel, geography)\n", "- Create a Power BI dashboard using the exported CSVs\n", "- Try time-based splits (train on earlier months, test on recent months)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 5}